---
title: SVM 衰落简史：优雅输给蛮力
description: 
date: 2026-01-23
scheduled: 2026-01-23
tags:
  - AI
layout: layouts/post.njk
---

以下记录一篇网文，作者是ConnectOnion的创始人Aaron，记录了SVM的兴衰。

>## 苏联数学家，看着对手拿诺奖

>2025年11月19日。

>Yann LeCun确认离开Meta。
>
>他在这家公司工作了12年。先是创建Facebook AI实验室，后来成为首席AI科学家。他和Geoffrey Hinton一起拿过图灵奖，被称为"深度学习教父"之一。
>
>现在，65岁的LeCun要创业了。他的新公司叫AMI Labs，估值35亿美元，在还没发布任何产品之前。
>
>他说大语言模型是"死胡同"。ChatGPT、Claude、GPT-5，这些都不是真正的智能。他要去做"世界模型"：能理解物理、有持久记忆、能规划复杂行动的AI。
>
>"不是预测下一个词，"他说，"理解世界。"
>
>十年前，在莫斯科的一场学术会议上，有一个人说过几乎一样的话。
>
>2015年，Yandex机器学习会议。
>
>一个79岁的老人站在台上。他穿着深色西装，头发花白，说话带着俄语口音。
>
>"想法和直觉，"他说，"要么来自上帝，要么来自魔鬼。区别是：上帝是聪明的，魔鬼不是。"
>
>台下坐着几百个机器学习研究者。有人在笑。有人皱眉。有人在想：这个老头在说什么？
>
>大数据和深度学习，都有蛮力的味道。
>
>这个老人叫Vladimir Vapnik。三十年前，他发明了一个算法，统治机器学习二十年。这个算法叫支持向量机，SVM。
>
>2015年，他的王国已经崩塌了。
>
>2010年代初，大学的机器学习课。老师讲到SVM的时候，声音都变了。
>
>"这是最优雅的算法，"她说，"数学上完美。"
>
>她在黑板上画了一条线，把两类点分开。然后她说：这条线不是随便画的。它是让两边距离最大的那条线。
>
>这个想法很简单。
>
>老师接着说：
>
>"找到这条线，需要解一个凸优化问题。凸优化意味着只有一个最优解。你不用担心陷入局部最优。"
>
>在机器学习的世界里，"有理论保证"是奢侈品。
>
>---
>
>## 莫斯科，1962
>
>Vladimir Vapnik二十六岁，刚开始在俄罗斯科学院控制科学研究所工作。他和同事Alexey Chervonenkis在研究一个问题：机器怎么从数据中学习？
>
>那时候是冷战时期。苏联的计算机又大又慢，和美国差距巨大。Vapnik不在乎硬件。他关心的是数学。
>
>他们发明了一个方法，叫"广义肖像法"。核心思想很简单：找到一个超平面，把不同类别的数据分开。
>
>SVM的前身。
>
>1964年，他们发表了第一篇论文。没有人在意。
>
>那时候机器学习还叫"模式识别"。主流是神经网络，早期的、原始的神经网络。大家都在试图模拟人脑。Vapnik和Chervonenkis的统计学方法，显得格格不入。
>
>理论。这套理论试图回答一个根本问题：给定有限的数据，我们能对未知情况做出多可靠的预测？
>
>1990年，苏联解体前夕，五十四岁的Vapnik离开了莫斯科。
>
>他去了美国。去了AT&T贝尔实验室。
>
>---
>
>## 贝尔实验室
>
>贝尔实验室的自适应系统研究部，是当时世界上最好的机器学习团队之一。
>
>部门主管叫Larry Jackel。团队里有Sara Solla、Yann LeCun、Leon Bottou、Isabelle Guyon。这些名字后来都成了传奇。
>
>Vapnik加入后，开始和一个丹麦女研究员合作。她叫Corinna Cortes，比Vapnik小二十五岁。
>
>Cortes后来回忆说，Vapnik有一种独特的思维方式。他不相信经验主义，相信数学。他觉得，一个算法如果没有理论保证，就不值得信任。
>
>SVM只能画直线，现实世界的数据往往是弯曲的。核技巧就是为了解决这个。
>
>不要在原始空间里找直线。把数据映射到更高维的空间，在那里画直线。
>
>这个想法很疯狂。有时候你需要把数据映射到无限维的空间。Vapnik证明了，你不需要真的去那个高维空间。你只需要一个"核函数"，它能帮你计算两个点在高维空>间里的距离。
>
>计算量不变，能力大幅提升。
>
>1995年，Vapnik和Cortes发表了那篇经典论文：《支持向量网络》。
>
>这篇论文后来被引用超过六万次。它奠定了SVM在机器学习界的地位。
>
>在接下来的十五年里，SVM几乎统治了机器学习。
>
>手写识别用SVM。人脸检测用SVM。文本分类、生物信息学、金融预测。到处都是SVM。
>
>LeCun和Vapnik的办公室只隔几米远。他们经常有一天，他们的老板Larry Jackel提议打赌。
>
>第一个赌约：2000年3月14日之前，会不会有数学理论解释为什么神经网络这么有效？Larry赌会有，Vapnik赌不会。
>
>第二个赌约：2005年3月14日之前，神经网络会不会被淘汰？Vapnik赌会，他坚信到那时候所有人都会用SVM。Larry赌不会。
>
>LeCun是见证人。两顿大餐作为赌注。
>
>第一个赌约，Vapnik赢了。2000年确实还没有数学理论能解释神经网络。
>
>第二个赌约，Vapnik输了。2005年神经网络还在，虽然不是主流。
>
>"Vapnik真的相信他的理论边界，"LeCun后来说，"他担心神经网络没有类似的容量控制方法。"
>
>LeCun的反驳是：计算复杂函数的能力，比容量控制更重要。
>
>2000年代，大多数人站在Vapnik这边。
>
>SVM解决的是分类问题。判断邮件是不是垃圾邮件，判断图片里是猫还是狗。
>
>传统的做法是这样的：
>
>1. 人工设计"特征"，比如图片的颜色分布、邮件的关键词
>2. 把特征输入SVM
>3. SVM找到最佳分类边界
>
>问题出在第一步。
>
>特征需要人来设计。算法的能力取决于人的聪明程度。你需要领域专家，需要大量试错，需要运气。
>
>深度学习省略了第一步。
>
>神经网络自己学习特征。你只需要把原始数据扔进去，它自己会找到有用的模式。
>
>两种完全不同的哲学。
>
>SVM说：给我好的特征，我给你最优的分类。
>
>深度学习说：给我足够的数据，我自己找特征虽然累，但至少可控。
>
>然后数据爆炸了。
>
>---
>
>## ImageNet 2012
>
>2012年10月，ImageNet大规模视觉识别挑战赛公布结果。
>
>一个叫Alex Krizhevsky的博士生，提交了一个深度卷积神经网络。他的导师是Geoffrey Hinton，一个在神经网络领域坚持了三十年的"异端"。
>
>这个网络有六千万个参数。它不是在大学的超级计算机上训练的。
>
>它是在Krizhevsky父母家的卧室里训练的。
>
>两块NVIDIA GTX 580显卡，插在一台普通电脑里。显卡风扇嗡嗡响。卧室里热得像桑拿房。Krizhevsky日复一日地调参数，看曲线，改代码，重新训练。
>
>训练了一年。
>
>他的模型错误率15.3%，第二名26.2%，差距超过十个百分点。
>
>"变得有点超现实，"Krizhevsky后来回忆。他从来不接受媒体采访，性格内向。但那几周他不得不面对潮水般的邮件。"收购要约很快就来了。很多邮件。"
>
>Hinton后来用一句话总结这个项目："Ilya认为我们应该做，Alex让它跑起来了，我拿了诺贝尔奖。"
>
>后来被称为AlexNet的这个模型证明了，给足够多的数据和计算力，神经网络可以碾压一切。
>
>同年欧洲计算机视觉会议上，Yann LeCun说了一句话：
>
>"这是计算机视觉历史上一个明确的转折点。"
>
>2014年，ImageNet的所有顶尖选手都变成了深度神经网络。
>
>Google花4400万美元收购了Hinton的空壳公司，实际上就是为了雇佣Hinton和他的两个学生。
>
>团队。
>
>SVM还在，已经退到了边缘。在大规模图像识别、语音识别、自然语言处理这些前沿领域，几乎看不到它的身影了。
>
>---
>
>## 为什么SVM输了
>
>SVM有一个致命问题：规模。
>
>SVM的训练时间和数据量的平方或立方成正比。一万个数据点可以处理，十万个勉强，一百万个、一千万个就跑不动了。
>
>核技巧虽然巧妙，但它需要计算所有数据点两两之间的"核函数值"。数据越多，计算量爆炸得越快。
>
>深度学习不一样。神经网络的训练可以用GPU并行。数据越多，虽然慢一点，但不会卡死。
>
>深度学习正好赶上了大数据时代。互联网产生了海量的图片、文本、视频。ImageNet数据集有一千四百多万张图片。这在2000年代是不可想象的。
>
>数据多了，深度学习就更强。SVM反而束手无策。
>
>规模不是唯一的原因。
>
>更根本的问题是：SVM是浅层的。
>
>Yann LeCun曾经评价说："SVMs在数学上很美，但归根结底，它们只是简单的两层系统。"
>
>两层系统能学到的东西是有限的。你可以用核技巧把它映射到高维空间，但本质上，它还是在找一个分界面。
>
>深度网络不一样。几十层、几百层。每一层都在学习更抽象的表示。从边缘，到纹理，到物体部件，到完整的语义概念。
>
>---
>
>## Vapnik的批评
>
>2015年Yandex会议上，Vapnik承认深度学习在实际问题上表现出色。但他觉得，这算不上真正的科学。
>
>"这是深度工程，"他说，"不是深度科学。"
>
>他的论点是：深度学习的从业者有很好的直觉和工程技能，但他们并不真正知道自己在做什么。且
>
>他和其他批评者指出一个事实：人类婴儿用很少的例子就能学会识别猫。深度学习需要几百万张猫的图片。
>
>"依赖海量数据的算法，"Vapnik说，"可能是在追求懒惰的解决方案，而不是发现学习的根本原理。"
>
>一边是Vapnik代表的数学派：优雅、理论保证、少量数据。另一边是Hinton代表的工程派：蛮力、大规模数据、实用主义。
>
>2015年，工程派已经赢了。
>
>---
>
>## Vapnik加入Facebook
>
>2014年11月，Facebook AI实验室宣布了一个高调的招聘。
>
>Vladimir Vapnik。SVM的发明者。加入了深度学习的大本营。
>
>实验室主管是Yann LeCun，他在贝尔实验室的老对手。
>
>Ronan Collobert，还有LeCun本人。这些人二十年前在贝尔实验室相邻的办公室里发明了卷积网络和SVM。
>
>二十年后，SVM的发明者加入了深度学习公司。
>
>Vapnik从来没解释过这个选择。
>
>Corinna Cortes现在是Google Research的副总裁。她离开贝尔实验室已经二十多年了。
>
>在一次采访中，她评价SVM和深度学习的关系：
>
>"支持向量机有坚实的理论基础和泛化保证，还有凸优化的优势。这和深度学习形成鲜明对比，深度学习目前没有明确的泛化保证，而且是高度非凸的优化问题。"
>
>她没说谁更好。只是陈述事实。
>
>近年来有研究者发现，当神经网络变得无限宽的时候，它在数学上等价于一种核方法，叫做神经切线核（Neural Tangent Kernel）。
>
>也就是说，在某种极限情况下，深度学习和SVM是这让一些研究者开始重新思考：Vapnik的理论，比我们想象的更接近真相。
>
>在实际应用中，SVM并没有完全消失。
>
>一家中型保险公司做过对比。用SVM做理赔分类，一周上线，准确率89%。用深度学习，六个月，12万美元云计算费用，准确率91%。准确率提升2%，成本增加七倍。
>
>"SVM在笔记本上训练只要几秒到几分钟，"一位机器学习工程师说，"我们可以在深度网络初始化的时间里测试五种不同的核函数。"
>
>2024年的一项调查显示：60%的软件安全从业者每周用大语言模型三到四次来管理漏洞，但他们仍然用基于SVM的检测工具做初步筛选。误报更少，不需要API调用，不需要云基础设施。
>
>2025年，当你数据不够多、需要可解释性、或者在处理高维结构化数据的时候，SVM仍然是一个可靠的选择。医疗诊断、金融风控、生物信息学，这些领域的研究者还在用SVM。虽然不时髦，但仍然可靠。
>
>---
>
>## 后来
>
>2017年，Vapnik获得IEEE冯·诺伊曼奖章。计算机科学的最高荣誉之一。
>
>颁奖词写道：表彰他在统计学习理论和支持向量机方面的开创性贡献。
>
>那时候ChatGPT还没有出现。Transformer刚刚发表。深度学习的浪潮才刚开始。
>
>2024年，Hinton拿到了诺贝尔物理学奖。颁奖词写的是"人工神经网络的基础性发现"。
>
>Krizhevsky离开了Google，离开了Dessa。2025年，他是蒙大拿州一家风投基金的合伙人。他几乎不接受采访。他的AlexNet论文被引用了超过17万次。
>
>Hinton用一句话总结AlexNet项目："Ilya认为我们应该做，Alex让它跑起来了，我拿了诺贝尔奖。"
>
>2025年11月，Vapnik八十九岁。他还在Meta AI然后LeCun离开了。
>
>他的老对手、他二十年前打赌的对象、深度学习的教父之一，突然说大语言模型是"死胡同"，然后出走创业。
>
>有人问LeCun为什么。他说Zuckerberg让研究团队加速开发Llama 4，结果发布的时候指标造假。"Results were fudged a little bit。"
>
>还有人问他对新上司的看法。Zuckerberg花143亿美元收购的Scale AI创始人王Alexandr，29岁。
>
>"年轻。"LeCun说。"没有经验。"
>
>"你不能告诉一个研究员该做什么。你当然不能告诉像我这样的研究员该做什么。"
>
>于是他走了。带着对"世界模型"的信仰：AI应该理解物理，不只是预测下一个词。
>
>现在Meta AI实验室里，Vapnik的老同事们走了一半。LeCun、Bottou、Collobert，当年贝尔实验室的传奇团队，现在各奔东西。
>
>Vapnik还在那里。
>
>他还在发表论文。在一次MIT的演讲中，他说了一句话："深度学习是胡扯。"
>
>台下安静了一会儿。
>
>1962年，莫斯科，一个二十六岁的数学家在研究一个问题：机器怎么从有限的数据中学习？
>
>2012年，多伦多，一个博士生在父母家的卧室里，用显卡训练一个六千万参数的模型。
>
>2025年，深度学习的教父说大语言模型是"死胡同"，出走创业去做"世界模型"。
>
>SVM的发明者还在问同一个问题。
>
>聪明和蛮力，是同一条路吗？
>
>---
>
>## Sources:
>
>- Vladimir Vapnik - Wikipedia
>- Corinna Cortes - ACM People
>- AlexNet: How It Transformed AI - IEEE Spectrum
>- Does Deep Learning Come from the Devil? - KDnuggets
>- The Forgotten Soviet Origins of the Support Vector Machine - Medium
>- Exclusive Interview with Yann LeCun - KDnuggets
>- How Vladimir Vapnik lost a fancy dinner in a bet - Yury Kashnitsky
>- Facebook's AI team hires Vladimir Vapnik - VentureBeat
>- SVM in 2025: Old School or Still Cool? - Medium
>- CHM Releases AlexNet Source Code - Computer History Museum
>- On the Equivalence between Neural Network and SVM - arXiv
>- Meta chief AI scientist Yann LeCun is leaving the company - CNBC
>- Yann LeCun is targeting a $3.5 billion valuation for his new startup - Fortune
>- AI godfather says Meta's new 29-year-old AI boss is 'inexperienced' - CNBC
>- Nobel Prize in Physics 2024 - Geoffrey Hinton
>- Geoffrey Hinton warns AI risks - CBS News
>- Alex Krizhevsky - Two Bear Capital