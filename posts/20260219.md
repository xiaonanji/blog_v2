---
title: 读书笔记 - 为什么LLM仅预测下一词就能涌现出高级能力？
description: 
date: 2026-02-18
scheduled: 2026-02-18
tags:
  - AI
  - Reading notes
layout: layouts/post.njk
---

**分享一篇小红书上看到的blog，解释是什么原因导致LLM涌现出高级智能。**

这件事其实困扰了学术界和工业界很久，哪怕是现在，<u>OpenAI</u> 内部也没人敢拍着胸脯说自己完全搞懂了其中的全部机理。

从最早的 <u>SVM</u>、<u>逻辑回归</u>，到后来的 <u>LSTM</u>、<u>BERT</u>，再到现在的 <u>GPT 系列</u>，眼看着模型参数从几万涨到几万亿。你可以去翻一个记录了各种技术变迁的[大模型237篇必读论文合集](https://mp.weixin.qq.com/s/CplXhesI8mGNqHQOyMXd9Q)，会发现一个有趣的现象：刚开始大家都觉得扯淡。一个做文本接龙的玩意，怎么就能写代码、做推理、甚至还能给你讲笑话？这不符合我们对"智能"二字的传统认知。我们以前觉得智能应该是结构化的，是有明确逻辑符号的，是类似数理逻辑推导那样的东西。

但现实给了我们一记响亮的耳光。

只要参数够大，数据够多，算力管饱，单单预测下一个词，就是能涌现出高级能力。

这背后的逻辑，其实非常硬核。我把这些年做算法、读论文、跟同行通宵debug聊出来的东西，揉碎了跟大伙说一说。别指望听到什么瑞士军刀之类的比喻，那些都是骗外行的，咱们直接看本质。

## 压缩即智能

这是目前最硬核的一个解释，也是我很推崇的一个观点。OpenAI 的 Ilya Sutskever 以前经常挂在嘴边。

你得这么想，预测下一个词，本质上是在对数据进行**无损压缩**。

想象一下，你要把整个互联网的文本数据压缩到一个模型里。这个模型的大小（参数量）远小于数据本身的大小。为了能准确还原（预测）出这些数据，模型必须被迫去学习数据内部的**规律**和**逻辑**。

如果我让你背诵圆周率，你死记硬背能背个几百位。但如果我让你预测圆周率的下一位，你光靠死记硬背是不行的，你必须掌握圆周率的计算公式。一旦你掌握了公式，你就能无限预测下去。

文本也是一样。

比如我在一段话里写：**张三把那把沉重的钥匙插进锁孔，用力一扭，只听咔嚓一声，门 \_\_ \_\_。**

你想预测这个空是 **<u>开</u> <u>了</u>**

这看起来简单。但模型要做到大概率预测出 **开** 这个字，它需要学到什么？

它得学会物理常识：钥匙插进锁孔扭动通常会导致锁打开。
它得学会语言习惯：咔嚓一声通常是机械结构动作的声音。
它得学会因果关系：扭动是因，门开是果。

这还只是最简单的例子。如果是一本侦探小说呢？

**警长看着地上的烟灰，突然想起来，死者生前从不抽烟，唯一的来访者是那个左撇子的 \_\_ \_\_ \_\_。**

要预测这里填 **<u>嫌</u> <u>疑</u> <u>人</u>** 或者具体的名字，模型必须具备**长期记忆**，必须具备**推理能力**，必须理解**反常即为妖**的逻辑。它得知道不抽烟的人地上有烟灰意味着有其他人来过，它得结合前文提到的左撇子特征去定位具体的人。

为了把这个**下一个词**预测准，模型被逼得没办法，只能去**理解**整个世界的运作规律。它不是想理解，它是为了降低预测错误率（Loss），**被迫**演化出了理解能力。

因为如果不理解，它就压缩不了这么多信息，它的预测准确率就上不去。

所以在海量数据的训练下，预测下一个词，等价于学习生成这些数据的**世界模型**。

我们以前做算法，总想着给模型设计各种特征，告诉它这是主语，这是谓语，这是因果关系。现在发现，根本不需要。你只要给它足够难的预测任务，它自己会在数千亿个参数里，构建出一种我们人类目前还看不懂，但确实有效的特征表示。

这种表示，比我们人工设计的要高维得多，也精细得多。

说到这，得提点稍微枯燥但很本质的东西。

语言模型本质上是在学习一个高维空间中的概率分布。

我们人类的语言、逻辑、知识，在这个高维空间里，不是杂乱无章的，而是分布在某些特定的<u>**流形**</u>（Manifold）上。

以前的词向量，比如<u>Word2Vec</u>，比较简单，它能学到**国王**和**男人**的距离，这就好比**女王**和**女人**的距离。这是一种简单的线性关系。

现在的 LLM，参数量巨大，它构建的这个语义空间非常复杂且稠密。

当你输入一段 prompt 的时候，你其实是在这个高维空间里确定了一个起点和方向。模型预测下一个词，就是在这个流形上沿着概率密度最高的路径走一步。这听起来很玄，其实底层全是矩阵运算。如果你对线性变换或空间映射感觉云里雾里的很抽象，可以去看下[3Blue1Brown的线性代数笔记.pdf](https://47saikyo.moe/Artificial%20Intelligence/Article/3Blue1Brown%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8%E3%80%8B%E7%AC%94%E8%AE%B0/)，它能帮你建立起直观的几何直觉，让你明白数据是如何在数学层面被压缩和理解的。

神奇的地方在于，这个流形结构，竟然把逻辑推理这种抽象能力也编码进去了。

有研究发现，当你让模型做数学题或者逻辑推理的时候，它内部的神经元激活模式，和它处理普通文本时是不一样的。它好像在那个高维空间里，找到了一条专门处理**逻辑变换**的通道。

**涌现**这种现象，很可能就是当参数量达到一定规模后，这个高维流形足够连续、足够平滑，使得模型可以从一个概念平滑地游走到另一个概念，哪怕这两个概念在训练数据里从来没有直接挨在一起出现过。

这就是**泛化**。

以前的小模型，流形是破碎的，全是坑坑洼洼，走两步就掉坑里了（输出乱码或废话）。大模型把这些坑填平了，把断路修通了。

我之前在项目里遇到过一个情况，我们用一个小模型去微调，教它写 SQL 语句。效果很差，稍微复杂点的嵌套查询就挂。后来换了个基座模型，参数量大了十倍，甚至没怎么专门教，它自己就能写出非常复杂的 join 查询。

为什么？

因为在大模型的语义空间里，**自然语言的逻辑**和**SQL 代码的逻辑**被映射到了相近的区域。它理解了**查询**这个概念的本质，而不仅仅是记住了 select * from table 这种死板的语法。

这种**概念层面的对齐**，是涌现高级能力的基础。

做工程的都知道 Scaling Laws。简单说就是，算力、数据、参数量，这三者指数级增加，模型的性能就会线性增长。

但这里有个 **相变（Phase Transition）** 的问题。要让这种相变发生，工程上的挑战是巨大的。你去一些看大厂在落地时关注的细节，比如可以去看下[字节大模型算法岗面试手册](https://zhuanlan.zhihu.com/p/1945774109968536366)里写的就能体会到，像这种从显存优化到分布式训练的坑，再到推理加速——这些**脏活累活**才是让 Scaling Laws 真正生效的幕后功臣。

很多能力，不是线性增长的。在模型很小的时候，它完全不会做加法。参数增加一点，它还是不会。直到参数突破某个临界值，它的加法能力突然从 0 跳到了 100。

这就像烧开水。0 度到 99 度，水都是液态，看起来没什么变化。到了 100 度，突然变成了气态。

对于 LLM 来说，预测下一个词的任务，在低算力下，模型主要靠**统计共现**。比如看到 **人工** 就接 **智能**，看到 **这** 就接 **是**。这完全是浅层的统计规律。

但当模型足够大，它发现光靠统计共现已经没法进一步降低 Loss 了。因为很多文本是需要深层逻辑才能预测准的。

这时候，模型内部可能发生了某种**相变**。它开始从**记忆模式**切换到**推理模式**。

有篇很有意思的论文讲 **Grokking（顿悟）** 现象。模型在训练集上已经过拟合了，准确率 100% 了，这时候继续训练，验证集准确率本来是不动的，突然在某个时间点，验证集准确率飙升。

这意味着模型抛弃了**死记硬背**的解法，突然找到了**通解（General Solution）**。

在预测下一个词这个任务的逼迫下，大模型被迫去寻找通解。因为数据量太大了，它根本背不下来所有组合。为了生存（降低 Loss），它必须学会推理算法。

这就像老师让你做一万道数学题。你脑子容量有限，背不下来每道题的答案。你最后被逼得没办法，只能学会了微积分公式。一旦学会了公式，你就不需要背答案了，而且你具备了做新题的能力。

这就是涌现的本质：**算法的内化**。

有一个点很多人容易忽略。现在的 LLM 之所以逻辑能力强，很大程度上是因为训练数据里包含了海量的**代码**。

代码和自然语言不一样。代码是严谨的逻辑链条。

**if A then B else C**。这种结构在代码里是绝对不能错的。缺少一个括号，程序就跑不通。

预测代码的下一个 token，比预测自然语言要难得多，也硬核得多。

在预测代码的过程中，模型被迫学会了**状态跟踪（State Tracking）**。比如它得记住 100 行前定义的变量 x 是什么类型，在第 101 行调用的时候才能预测对。

这种**长距离的依赖关系**和**严格的逻辑执行**能力，被模型学到后，迁移到了自然语言任务上。

所以你会发现，现在的模型写作文理特别清晰，第一点、第二点、第三点，逻辑严丝合缝。这很可能就是从代码里学来的 **思维链（Chain of Thought）** 能力。

我有个做大模型预训练的朋友跟我聊过，他们在训练数据里剔除掉代码后，模型的推理能力出现了肉眼可见的下降。这就反向证明了，预测代码的 next token，是构建高级逻辑能力的关键一环。

我们再换个角度。

我们可以把 LLM 看作是一个巨大的 **<u>贝叶斯推理机</u>**。

每一次预测下一个词，其实都是在做一次贝叶斯更新。

**P(Next Word | Context)**。

这个 Context（上下文），包含了你给它的 Prompt，也包含了它自己刚刚生成的词。

随着生成的词越来越多，Context 包含的信息量就越大，对后续内容的约束就越强。

人类思考问题的过程，其实也是类似的。

你想想你写代码或者写文章的时候。你心里有个模糊的想法（意图）。你写下第一个字。这个字会限制你第二个字的选择。写完第一句，你必须在第一句的基础上写第二句。

你的思想在文字的生成过程中逐渐成型。

很多时候，我们自己都不知道自己下一句要具体说什么，直到我们把上一句说完。

**语言不仅仅是思想的载体，语言本身就是思想的过程。**

LLM 完美地模拟了这个过程。

它不需要预先规划好整个宏大叙事。它只需要在每一步，基于当前的上下文，做出最合理的推断。当每一步都走得合理时，整体看下来，就涌现出了宏大的逻辑和智慧。

这有点像生物进化。每一步变异和选择都只是为了当下的生存，没有长远规划。但经过亿万年的迭代，竟然涌现出了人类大脑这么复杂的结构。

Next Token Prediction，就是进化的选择压力。Loss Function，就是自然选择的剪刀。

还有个问题，为什么是文本预测？为什么图像预测没有这么明显的逻辑涌现？

因为人类的智慧，绝大部分都浓缩在文本里。

文字是人类思维的高维投影。我们几千年的历史、科学、哲学、情感，都编码在文字里了。

图像虽然信息量大，但它是**感知**层面的。一只猫的图片，像素点之间更多是空间相关性。

而文字，是**符号**层面的。文字符号之间，充满了**逻辑**、**因果**、**抽象**关系。

预测文本，本质上是在学习人类的**思维方式**。

你在训练模型预测**苏格拉底是人，所有人都有一死，所以苏格拉底\_\_ \_\_**。

模型要填 **<u>会</u> <u>死</u>**。这不仅仅是词语接龙，这是三段论逻辑。

互联网上的文本数据，包含了人类所有的三段论、归纳法、演绎法、反证法。模型把这些都学去了。

所以，与其说模型涌现了智能，不如说模型**复刻**了人类群体智慧的思维模式。它是一面镜子，反射的是全人类在互联网上留下的思维痕迹。

前段时间我们搞了个运维机器人的项目。以前的方法是写一大堆正则匹配规则，去提取 Log 里的错误信息。累死人，而且一旦报错格式变了就失效。

后来我们试着用 LLM。

我们把报错的那一段 Log 扔给它，让它预测接下来的分析。

注意，我们没有专门教它怎么修服务器。

但是，模型在预训练阶段，看过 StackOverflow，看过 GitHub 上的 issue 讨论，看过各种技术博客。

当它看到一段特定的 Java 堆栈溢出报错时，它预测下一个词的概率分布里，自动浮现出了**内存泄漏**、**HashMap**、**死循环**这些概念。

它给出的建议是：**检查一下代码里是不是在遍历 Map 的时候进行了删除操作。**

这真的是惊掉下巴。它不仅仅是匹配到了错误，它理解了这个错误背后的**代码逻辑**。

它之所以能做到，是因为在它的训练数据里，无数个程序员在预测下一个词的时候，把**报错**和**成因**关联起来了。模型把这种关联压缩进了参数里。

当我们输入报错，模型就在它那高维的语义空间里，顺着概率流形，找到了成因所在的区域。

很多人觉得，仅仅预测下一个词，这也太简单了吧？智能怎么可能这么简单？

这种直觉是错的。

我们太低估 **预测** 这个任务的难度了。

在一个开放的世界里，实现完美的预测，等于全知全能。

你想想，如果我要完美预测明天的股票价格（这也是一种时序数据，类似于 Next Token），我需要知道什么？

我需要知道全球的政治局势、经济政策、公司财报、甚至大众的心理状态。

任何一个信息的缺失，都会导致预测的不完美。

所以，追求极致的预测准确率，就是在这个过程中倒逼模型去建模整个世界。

LLM 现在的能力还远没有到顶。现在的 Loss 还没有降到 0（也不可能降到 0，因为语言有内在的随机性），但还有很大的下降空间。

只要 Loss 还能降，模型对世界的理解就还能更深。

虽然我是做技术的，但看着这玩意儿一天天变强，心里也犯嘀咕。

我们是不是无意中触碰到了智慧的本质？

也许人类大脑的工作方式，在底层机制上，真的就是一个不断进行 Next Token Prediction 的生物神经网络？

我们听到一句话，大脑自动预测下一句。我们看到一个动作，大脑自动预测后果。

那种所谓的**灵感**、**顿悟**、**意识**，可能只是大规模神经网络在处理复杂预测任务时，涌现出来的高层宏观现象。

如果真的是这样，那我们现在走的这条路，可能真的就是通往 AGI（通用人工智能）的康庄大道。

当然，现在还有很多问题。幻觉问题、逻辑一致性问题、长窗口记忆问题。

但这些更像是工程问题，而不是原理上的死胡同。

所以为什么预测下一个词能涌现智能?

1.**压缩即理解**：为了在海量数据上实现高准确率的预测，模型被迫学会了数据背后的生成规律（也就是世界知识和逻辑）。

2.**高维语义空间**：模型构建了一个包含了逻辑、因果、概念的稠密流形结构，推理变成了在这个空间里的路径游走。

3.**Scaling Laws 与相变**：量变引起质变，参数量大到一定程度，模型从统计记忆转向了算法推演。

4.**代码数据的加持**：代码训练增强了模型的严谨逻辑和长程依赖能力。

5.**文本的特殊性**：文本是人类思维的高保真投影，学文本就是学思维。

这事儿没什么玄学的，全是数学和算力堆出来的奇迹。但正是因为是数学，才让人觉得更震撼。因为这意味着，智能是可以被计算的，是可以被我们亲手制造出来的。

我们这代工程师，有幸见证甚至参与这个过程，说实话挺带劲的。

不管未来这东西会发展成什么样，至少现在，别把它当成一个简单的 **自动补全工具**。它是一个正在努力通过预测下一个字，来试图理解我们这个嘈杂、混乱但又充满逻辑的世界的 **庞然大物**。